{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd564fd4",
   "metadata": {},
   "source": [
    "# Train-Test Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d734a2",
   "metadata": {},
   "source": [
    "Train test frameworks are an essential part of Machine Learning (ML) workflows. Splitting the data into training and testing sets is a critical step in building ML models. In this tutorial, we will cover three popular train test frameworks: Holdout split, K-fold split, and Leave-One-Out split using the Scikit-learn library.\n",
    "\n",
    "## Holdout Split\n",
    "\n",
    "Holdout split is a straightforward method for splitting data into training and testing sets. It randomly divides the data into two sets, a training set and a testing set. We use the training set to train the model and the testing set to evaluate its performance.\n",
    "\n",
    "Here is an example of how to use the train_test_split function from Scikit-learn to split the data into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e4c3cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# generate random data\n",
    "X = np.random.normal(0, 1, 20).reshape(10, 2)\n",
    "y = np.random.normal(0, 1, 10)\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b74923",
   "metadata": {},
   "source": [
    "In the example above, we first generated a random dataset of shape (10, 2) for features X and shape (10,) for the target variable y. We then used the train_test_split function from Scikit-learn to split the data into training and testing sets. We set the test_size parameter to 0.3, which means that 30% of the data is used for testing, and the remaining 70% is used for training. We also set the random_state parameter to 42 to ensure reproducibility of results.\n",
    "\n",
    "After splitting the data, we can print the shapes of the training and testing sets using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0859f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7, 2)\n",
      "X_test shape: (3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875abf2b",
   "metadata": {},
   "source": [
    "## Holdout Split without Shuffle\n",
    "\n",
    "In some cases, we may not want to shuffle the data before splitting it into training and testing sets. This is useful, for example, when working with time-series data or data with a specific structure. In this case, we can set the shuffle parameter to False in the train_test_split function. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e26590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: [[ 0.32846033 -2.27171679]\n",
      " [-0.66579653  0.60807083]\n",
      " [ 0.0960338   0.77509136]\n",
      " [-1.71511511  0.31740218]\n",
      " [-0.48028649  0.33100787]\n",
      " [-0.35175208  0.38656938]\n",
      " [ 0.69733549  0.14115955]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# generate random data\n",
    "X = np.random.normal(0, 1, 20).reshape(10, 2)\n",
    "y = np.random.normal(0, 1, 10)\n",
    "\n",
    "# split data into training and testing sets without shuffle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
    "\n",
    "print(\"X_train:\", X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a861e17",
   "metadata": {},
   "source": [
    "## K-fold Cross-Validation\n",
    "\n",
    "K-fold cross-validation is a more advanced method of splitting data into training and testing sets. In this method, the data is divided into k equal parts (folds), and the model is trained k times. Each time, one of the k folds is used as the testing set, and the remaining k-1 folds are used as the training set. The performance of the model is then averaged over the k runs.\n",
    "\n",
    "Here is an example of how to use the KFold function from Scikit-learn to perform K-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "945ec0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [2 3 4 5 6 7 8 9]\n",
      "Test indices: [0 1]\n",
      "Train indices: [0 1 4 5 6 7 8 9]\n",
      "Test indices: [2 3]\n",
      "Train indices: [0 1 2 3 6 7 8 9]\n",
      "Test indices: [4 5]\n",
      "Train indices: [0 1 2 3 4 5 8 9]\n",
      "Test indices: [6 7]\n",
      "Train indices: [0 1 2 3 4 5 6 7]\n",
      "Test indices: [8 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# generate random data\n",
    "X = np.random.normal(0, 1, 20).reshape(10, 2)\n",
    "y = np.random.normal(0, 1, 10)\n",
    "\n",
    "# instantiate KFold with k=5\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# iterate over train_index and test_index in kf.split(X) and print them\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"Train indices:\", train_index)\n",
    "    print(\"Test indices:\", test_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55958c",
   "metadata": {},
   "source": [
    "In the example above, we first generated a random dataset of shape (10, 2) for features X and shape (10,) for the target variable y. We then instantiated the KFold function from Scikit-learn with n_splits=5, which means that we will use 5 folds for cross-validation.\n",
    "\n",
    "We then used a for loop to iterate over the training and testing indices for each fold. The kf.split(X) function returns the indices of the training and testing sets for each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2b1ce1",
   "metadata": {},
   "source": [
    "## K-fold Cross-Validation with Shuffle\n",
    "\n",
    "By default, the KFold function performs cross-validation without shuffling the data. However, we can shuffle the data by setting the shuffle parameter to True. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "681a3cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [2 3 4 5 6 7 8 9]\n",
      "Test indices: [0 1]\n",
      "Train indices: [0 1 3 4 5 6 8 9]\n",
      "Test indices: [2 7]\n",
      "Train indices: [0 1 2 4 5 6 7 9]\n",
      "Test indices: [3 8]\n",
      "Train indices: [0 1 2 3 5 6 7 8]\n",
      "Test indices: [4 9]\n",
      "Train indices: [0 1 2 3 4 7 8 9]\n",
      "Test indices: [5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# generate random data\n",
    "X = np.random.normal(0, 1, 20).reshape(10, 2)\n",
    "y = np.random.normal(0, 1, 10)\n",
    "\n",
    "# instantiate KFold with k=5 and shuffle=True\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# iterate over train_index and test_index in kf.split(X) and print them\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"Train indices:\", train_index)\n",
    "    print(\"Test indices:\", test_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c87de1f",
   "metadata": {},
   "source": [
    "In the example above, we set the shuffle parameter to True when instantiating the KFold function. This will shuffle the data before performing cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd7c0d",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross-Validation\n",
    "\n",
    "The Leave-One-Out (LOO) method is a special case of K-fold cross-validation, where k is equal to the number of data points. In other words, each data point is used as the testing set once, and the remaining data points are used as the training set.\n",
    "\n",
    "Here is an example of how to use the LeaveOneOut function from Scikit-learn to perform LOO cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db4719d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [1 2 3 4 5 6 7 8 9]\n",
      "Test indices: [0]\n",
      "Train indices: [0 2 3 4 5 6 7 8 9]\n",
      "Test indices: [1]\n",
      "Train indices: [0 1 3 4 5 6 7 8 9]\n",
      "Test indices: [2]\n",
      "Train indices: [0 1 2 4 5 6 7 8 9]\n",
      "Test indices: [3]\n",
      "Train indices: [0 1 2 3 5 6 7 8 9]\n",
      "Test indices: [4]\n",
      "Train indices: [0 1 2 3 4 6 7 8 9]\n",
      "Test indices: [5]\n",
      "Train indices: [0 1 2 3 4 5 7 8 9]\n",
      "Test indices: [6]\n",
      "Train indices: [0 1 2 3 4 5 6 8 9]\n",
      "Test indices: [7]\n",
      "Train indices: [0 1 2 3 4 5 6 7 9]\n",
      "Test indices: [8]\n",
      "Train indices: [0 1 2 3 4 5 6 7 8]\n",
      "Test indices: [9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# generate random data\n",
    "X = np.random.normal(0, 1, 20).reshape(10, 2)\n",
    "y = np.random.normal(0, 1, 10)\n",
    "\n",
    "# instantiate LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# iterate over train_index and test_index in loo.split(X) and print them\n",
    "for train_index, test_index in loo.split(X):\n",
    "    print(\"Train indices:\", train_index)\n",
    "    print(\"Test indices:\", test_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23acff9",
   "metadata": {},
   "source": [
    "In the example above, we first generated a random dataset of shape (10, 2) for features X and shape (10,) for the target variable y. We then instantiated the LeaveOneOut function from Scikit-learn.\n",
    "\n",
    "We used a for loop to iterate over the training and testing indices for each fold. The loo.split(X) function returns the indices of the training and testing sets for each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ab0d4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have covered three popular train test frameworks: Holdout split, K-fold split, and Leave-One-Out split using the Scikit-learn library. These methods are essential for evaluating the performance of ML models and for avoiding overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bebfa82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
